{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO3W1mKwuZB7"
      },
      "source": [
        "**MODELOS DE EMBEDDINGS BASADOS EN WORD2VEC**\n",
        "\n",
        "*John Atkinson*\n",
        "\n",
        "Este programa utiliza  crea y utiliza modelos de embeddings de lenguaje basado en m√®todos del tipo Word2Vec.\n",
        "\n",
        "Primero, necesitamos instalar algunos paquetes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CfmS1KKiun1t"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in c:\\users\\karin\\anaconda3\\lib\\site-packages (3.7.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (8.2.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (21.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (2.26.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: setuptools in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (58.0.4)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (1.20.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.8.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.5 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: colorama in c:\\users\\karin\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.3)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
            "Collecting es-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
            "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from es-core-news-sm==3.7.0) (3.7.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.26.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.20.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (21.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: setuptools in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (58.0.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.5.2)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.9.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.11.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.8.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.5 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.14.5)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.26.7)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: colorama in c:\\users\\karin\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.0.3)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.1)\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "Requirement already satisfied: gensim in c:\\users\\karin\\anaconda3\\lib\\site-packages (4.3.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from gensim) (1.7.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from gensim) (6.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from gensim) (1.20.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download es_core_news_sm\n",
        "%pip install gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZpOLHcVuo2o"
      },
      "source": [
        "Montamos nuestro *Drive* donde se encuentra la carpeta CORPUS con documentos separados por tema:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UdMswDzuwB7"
      },
      "source": [
        "Debemos importar algunas bibliotecas  y utilitarios:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "n3ZXsaVjuwrC"
      },
      "outputs": [],
      "source": [
        "import es_core_news_sm\n",
        "from string import punctuation\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import regex\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.spatial.distance import cosine\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import unicodedata\n",
        "import spacy\n",
        "from spacy.lang.es.stop_words import STOP_WORDS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ew3Tgj9PuOjW",
        "outputId": "27c69e51-6de5-4dcc-8dbd-9f01a0068d05"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/gdrive/MyDrive/CORPUS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "file_paths = glob.glob(\"corpus/*\")\n",
        "corpus = []\n",
        "\n",
        "for file_path in file_paths:\n",
        "    with open(file_path, 'r') as file:\n",
        "        content = file.read()\n",
        "        corpus.append(content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCdfEFEfvT7Q"
      },
      "source": [
        "Primero, definimos la funci√≥n **EntrenarModelo(oraciones,NombreModelo)**, que permite entrenar un modelo Word2Vec a partir de un conjunto de oraciones extra√≠da desde un corpus. El modelo generado se graba en la carpeta **NombreModelo**. Asumimos que la ventaja de entrenamiento es 2 ($windows=2$) y que el n√∫mero de dimensiones o tama√±o del vector es 8 ($size=8$). Note que se debe encontrar el mejor tama√±o del embedding.\n",
        "\n",
        "Luego, definimos una funci√≥n **CargarModelo(NombreModelo)**, que nos permitir√° cargar un modelo cuando sea necesario.\n",
        "\n",
        "Note que no necesariamente debemos entrenar el modelo nosotros mismos. Podr√≠amos cargar un modelo de embeddings que ya ha sido entrenado por alguien m√°s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xvDmFlnDwLE4"
      },
      "outputs": [],
      "source": [
        "def EntrenarModelo(oraciones,NombreModelo):\n",
        "    model = Word2Vec(oraciones, vector_size=300, window=2, min_count=1)\n",
        "    model.save(NombreModelo)\n",
        "\n",
        "def CargarModelo(NombreModelo):\n",
        "   modelo = Word2Vec.load(NombreModelo)\n",
        "   vocabulario = [term for term in modelo.wv.key_to_index]\n",
        "   return(modelo,vocabulario)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-qH4YzKz-mg"
      },
      "source": [
        "Adem√°s, necesitaremos una funci√≥n **ObtenerEmbeddingOracion(modelo, oracion)**, que nos permitir√° obtener el embedding (vector) de una oraci√≥n desde un modelo entrenado. El embedding de una oraci√≥n es simplemente el vector promedio de cada una de las palabras de la oraci√≥n:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5iOz77PFz88a"
      },
      "outputs": [],
      "source": [
        "def ObtenerEmbeddingOracion(modelo, oracion):\n",
        "   Lista_vectores = []\n",
        "   for w in Tokenizar(oracion):\n",
        "       # Verificar que la palabra w exista en el modelo\n",
        "       try:\n",
        "           modelo.wv[w]\n",
        "       except KeyError:\n",
        "           continue\n",
        "       # Obtener vector de la palabra\n",
        "       vec = modelo.wv[w]\n",
        "       Lista_vectores.append(vec)\n",
        "   embedding_palabras = np.array(Lista_vectores)\n",
        "   if (len(embedding_palabras) > 0):\n",
        "        embedding_oracion = embedding_palabras.mean(axis=0)\n",
        "   else:\n",
        "        embedding_oracion = np.zeros(modelo.vector_size)\n",
        "   return(embedding_oracion)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZj5KFfkyQnX"
      },
      "source": [
        "Ahora, utilizamos algunas funciones de preprocesamiento:\n",
        "(lematizar, eliminar stopwords y tokenizar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EA-A89OTyiy1"
      },
      "outputs": [],
      "source": [
        "def PreProcesarOraciones(textos):\n",
        "    texto_limpio = []\n",
        "    for texto in textos:\n",
        "        if len(texto)!=0:\n",
        "          texto = regex.sub(' +', ' ', texto)\n",
        "          tokens = Tokenizar(texto)\n",
        "          texto_limpio.append(tokens)\n",
        "    return(texto_limpio)\n",
        "\n",
        "def Tokenizar(oracion):\n",
        "    doc = nlp(oracion)\n",
        "    tokens = [palabra.text for palabra in doc]\n",
        "    return(tokens)\n",
        "\n",
        "def ProcesarTexto(oracion):\n",
        "    doc = nlp(oracion)\n",
        "    tokens = [palabra.lemma_ for palabra in doc if not palabra.is_stop]\n",
        "    return tokens\n",
        "\n",
        "def CrearCorpus(path):\n",
        "  directorio = os.listdir(path)\n",
        "  corpus = []\n",
        "  doc_id = []\n",
        "  for filename  in directorio:\n",
        "     texto = open(path+filename,'r',encoding=\"latin-1\").read()\n",
        "     corpus.append(texto)\n",
        "     doc_id.append(filename)\n",
        "  return(corpus, doc_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Szvub8wifLU4"
      },
      "source": [
        "En caso de ser necesario definimos una funci√≥n que permite convertir una lista a un diccionario, de modo de poder acceder por clave (y no por √≠ndice):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-vqcwQ55fRBc"
      },
      "outputs": [],
      "source": [
        "def CrearDiccionario(lista,claves):\n",
        "   dicc = {}\n",
        "   for  v in range(0,len(claves)):\n",
        "      dicc[claves[v]] = lista[v]\n",
        "   return(dicc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tTIjw2vzFMG"
      },
      "source": [
        "Ahora, ejecutamos nuestro programa principal con algunas incializaciones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FK34lW6izLAL"
      },
      "outputs": [],
      "source": [
        "PATH = \"corpus/\"\n",
        "nlp          = es_core_news_sm.load()\n",
        "corpus, docID = CrearCorpus(PATH)\n",
        "oraciones    =  PreProcesarOraciones(corpus)\n",
        "CorpusConClave  = CrearDiccionario(corpus,docID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DAyV3uSzSRv"
      },
      "source": [
        "Entrenamos el modelo en base a las oraciones generadas previamente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uIh2CDtDzY_o"
      },
      "outputs": [],
      "source": [
        "EntrenarModelo(oraciones,'mi_word2vec')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovHOhFoJzbEQ"
      },
      "source": [
        "Luego, cargamos el modelo entrenado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NWBtEpdgzlYA"
      },
      "outputs": [],
      "source": [
        "modelo, vocabulario = CargarModelo('mi_word2vec')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3OglWudfpPk"
      },
      "source": [
        "Podemos, obtener el embedding de alguna palabra:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-fqrq_W0sFw"
      },
      "source": [
        "Una vez que tenemos nuestro modelo cargado, podemos realizar diferentes tareas sobre los vectores de palabras u oraciones.\n",
        "\n",
        "Por ejemplo, podemos determinar la *cercan√≠a* entre dos documentos del corpus. Para ello:\n",
        "\n",
        "1.   Tomamos el texto de cada documento.\n",
        "2.   Obtenemos sus respectivos vectores (embeddings).\n",
        "3.   Calculamos la distancia **coseno**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HwRvrMqgG-d"
      },
      "source": [
        "Note que la calidad del resultado depende de la cercan√≠a real de los documentos!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Solicitamos al Usuario las palabras para comparar y mostramos las primeras 30 similitudes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documento: 149691.txt, Similitud: 0.6409827470779419\n",
            "Documento: 149471.txt, Similitud: 0.625017523765564\n",
            "Documento: 150206.txt, Similitud: 0.6183123588562012\n",
            "Documento: 97464.txt, Similitud: 0.6145167946815491\n",
            "Documento: 152647.txt, Similitud: 0.6103097200393677\n",
            "Documento: 135384.txt, Similitud: 0.6055464744567871\n",
            "Documento: 84418.txt, Similitud: 0.6054936051368713\n",
            "Documento: 151662.txt, Similitud: 0.6044436693191528\n",
            "Documento: 82199.txt, Similitud: 0.6034389138221741\n",
            "Documento: 85692.txt, Similitud: 0.6033058762550354\n",
            "Documento: 103681.txt, Similitud: 0.6029363870620728\n",
            "Documento: 82440.txt, Similitud: 0.6006869673728943\n",
            "Documento: 103651.txt, Similitud: 0.6001328825950623\n",
            "Documento: 74127.txt, Similitud: 0.5995113253593445\n",
            "Documento: 134150.txt, Similitud: 0.5983396172523499\n",
            "Documento: 152727.txt, Similitud: 0.5983177423477173\n",
            "Documento: 151583.txt, Similitud: 0.5976815819740295\n",
            "Documento: 150645.txt, Similitud: 0.5976508855819702\n",
            "Documento: 151937.txt, Similitud: 0.597638726234436\n",
            "Documento: 72656.txt, Similitud: 0.5969370603561401\n",
            "Documento: 149210.txt, Similitud: 0.5945630669593811\n",
            "Documento: 152444.txt, Similitud: 0.5944116711616516\n",
            "Documento: 81050.txt, Similitud: 0.5930393934249878\n",
            "Documento: 164282.txt, Similitud: 0.5930287837982178\n",
            "Documento: 99120.txt, Similitud: 0.5917245149612427\n",
            "Documento: 152967.txt, Similitud: 0.590538740158081\n",
            "Documento: 137858.txt, Similitud: 0.5903235673904419\n",
            "Documento: 150322.txt, Similitud: 0.5895110368728638\n",
            "Documento: 81517.txt, Similitud: 0.5888689160346985\n",
            "Documento: 150341.txt, Similitud: 0.5883181691169739\n"
          ]
        }
      ],
      "source": [
        "\n",
        "query = input(\"Ingresa tus palabras para comparar: \")\n",
        "\n",
        "\n",
        "query_embedding = ObtenerEmbeddingOracion(modelo, query)\n",
        "\n",
        "\n",
        "similitud_puntajes = []\n",
        "for documentos in corpus:\n",
        "    documentos_embedding = ObtenerEmbeddingOracion(modelo, documentos)\n",
        "    similitud = 1 - cosine(query_embedding, documentos_embedding)\n",
        "    similitud_puntajes.append(similitud)\n",
        "\n",
        "\n",
        "top_indices = np.argsort(similitud_puntajes)[::-1][:30]\n",
        "\n",
        "\n",
        "for index in top_indices:\n",
        "    documentos_name = docID[index]\n",
        "    similitud = similitud_puntajes[index]\n",
        "    print(f\"Documento: {documentos_name}, Similitud: {similitud}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
