{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO3W1mKwuZB7"
      },
      "source": [
        "**MODELOS DE EMBEDDINGS BASADOS EN WORD2VEC**\n",
        "\n",
        "*John Atkinson*\n",
        "\n",
        "Este programa utiliza  crea y utiliza modelos de embeddings de lenguaje basado en mètodos del tipo Word2Vec.\n",
        "\n",
        "Primero, necesitamos instalar algunos paquetes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CfmS1KKiun1t"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in c:\\users\\karin\\anaconda3\\lib\\site-packages (3.7.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (8.2.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (21.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (2.26.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: setuptools in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (58.0.4)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (1.20.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.8.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.5 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: colorama in c:\\users\\karin\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.3)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
            "Collecting es-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
            "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from es-core-news-sm==3.7.0) (3.7.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.26.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.20.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (21.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: setuptools in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (58.0.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.5.2)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.9.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.11.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.8.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.5 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.14.5)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.26.7)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: colorama in c:\\users\\karin\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.0.3)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "Requirement already satisfied: gensim in c:\\users\\karin\\anaconda3\\lib\\site-packages (4.3.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from gensim) (1.7.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from gensim) (6.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\karin\\anaconda3\\lib\\site-packages (from gensim) (1.20.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download es_core_news_sm\n",
        "%pip install gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZpOLHcVuo2o"
      },
      "source": [
        "Montamos nuestro *Drive* donde se encuentra la carpeta CORPUS con documentos separados por tema:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UdMswDzuwB7"
      },
      "source": [
        "Debemos importar algunas bibliotecas  y utilitarios:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "n3ZXsaVjuwrC"
      },
      "outputs": [],
      "source": [
        "import es_core_news_sm\n",
        "from string import punctuation\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import regex\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.spatial.distance import cosine\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import unicodedata\n",
        "import spacy\n",
        "from spacy.lang.es.stop_words import STOP_WORDS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ew3Tgj9PuOjW",
        "outputId": "27c69e51-6de5-4dcc-8dbd-9f01a0068d05"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/gdrive/MyDrive/CORPUS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "file_paths = glob.glob(\"corpus/*\")\n",
        "corpus = []\n",
        "\n",
        "for file_path in file_paths:\n",
        "    with open(file_path, 'r') as file:\n",
        "        content = file.read()\n",
        "        corpus.append(content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCdfEFEfvT7Q"
      },
      "source": [
        "Primero, definimos la función **EntrenarModelo(oraciones,NombreModelo)**, que permite entrenar un modelo Word2Vec a partir de un conjunto de oraciones extraída desde un corpus. El modelo generado se graba en la carpeta **NombreModelo**. Asumimos que la ventaja de entrenamiento es 2 ($windows=2$) y que el número de dimensiones o tamaño del vector es 8 ($size=8$). Note que se debe encontrar el mejor tamaño del embedding.\n",
        "\n",
        "Luego, definimos una función **CargarModelo(NombreModelo)**, que nos permitirá cargar un modelo cuando sea necesario.\n",
        "\n",
        "Note que no necesariamente debemos entrenar el modelo nosotros mismos. Podríamos cargar un modelo de embeddings que ya ha sido entrenado por alguien más."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xvDmFlnDwLE4"
      },
      "outputs": [],
      "source": [
        "def EntrenarModelo(oraciones,NombreModelo):\n",
        "    model = Word2Vec(oraciones, vector_size=300, window=2, min_count=1)\n",
        "    model.save(NombreModelo)\n",
        "\n",
        "def CargarModelo(NombreModelo):\n",
        "   modelo = Word2Vec.load(NombreModelo)\n",
        "   vocabulario = [term for term in modelo.wv.key_to_index]\n",
        "   return(modelo,vocabulario)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-qH4YzKz-mg"
      },
      "source": [
        "Además, necesitaremos una función **ObtenerEmbeddingOracion(modelo, oracion)**, que nos permitirá obtener el embedding (vector) de una oración desde un modelo entrenado. El embedding de una oración es simplemente el vector promedio de cada una de las palabras de la oración:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5iOz77PFz88a"
      },
      "outputs": [],
      "source": [
        "def ObtenerEmbeddingOracion(modelo, oracion):\n",
        "   Lista_vectores = []\n",
        "   for w in Tokenizar(oracion):\n",
        "       # Verificar que la palabra w exista en el modelo\n",
        "       try:\n",
        "           modelo.wv[w]\n",
        "       except KeyError:\n",
        "           continue\n",
        "       # Obtener vector de la palabra\n",
        "       vec = modelo.wv[w]\n",
        "       Lista_vectores.append(vec)\n",
        "   embedding_palabras = np.array(Lista_vectores)\n",
        "   if (len(embedding_palabras) > 0):\n",
        "        embedding_oracion = embedding_palabras.mean(axis=0)\n",
        "   else:\n",
        "        embedding_oracion = np.zeros(modelo.vector_size)\n",
        "   return(embedding_oracion)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZj5KFfkyQnX"
      },
      "source": [
        "Ahora, utilizamos algunas funciones de preprocesamiento:\n",
        "(lematizar, eliminar stopwords y tokenizar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EA-A89OTyiy1"
      },
      "outputs": [],
      "source": [
        "def PreProcesarOraciones(textos):\n",
        "    texto_limpio = []\n",
        "    for texto in textos:\n",
        "        if len(texto)!=0:\n",
        "          texto = regex.sub(' +', ' ', texto)\n",
        "          tokens = Tokenizar(texto)\n",
        "          texto_limpio.append(tokens)\n",
        "    return(texto_limpio)\n",
        "\n",
        "def Tokenizar(oracion):\n",
        "    doc = nlp(oracion)\n",
        "    tokens = [palabra.text for palabra in doc]\n",
        "    return(tokens)\n",
        "\n",
        "def ProcesarTexto(oracion):\n",
        "    doc = nlp(oracion)\n",
        "    tokens = [palabra.lemma_ for palabra in doc if not palabra.is_stop]\n",
        "    return tokens\n",
        "\n",
        "def CrearCorpus(path):\n",
        "  directorio = os.listdir(path)\n",
        "  corpus = []\n",
        "  doc_id = []\n",
        "  for filename  in directorio:\n",
        "     texto = open(path+filename,'r',encoding=\"latin-1\").read()\n",
        "     corpus.append(texto)\n",
        "     doc_id.append(filename)\n",
        "  return(corpus, doc_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Szvub8wifLU4"
      },
      "source": [
        "En caso de ser necesario definimos una función que permite convertir una lista a un diccionario, de modo de poder acceder por clave (y no por índice):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-vqcwQ55fRBc"
      },
      "outputs": [],
      "source": [
        "def CrearDiccionario(lista,claves):\n",
        "   dicc = {}\n",
        "   for  v in range(0,len(claves)):\n",
        "      dicc[claves[v]] = lista[v]\n",
        "   return(dicc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tTIjw2vzFMG"
      },
      "source": [
        "Ahora, ejecutamos nuestro programa principal con algunas incializaciones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FK34lW6izLAL"
      },
      "outputs": [],
      "source": [
        "PATH = \"corpus/\"\n",
        "nlp          = es_core_news_sm.load()\n",
        "corpus, docID = CrearCorpus(PATH)\n",
        "oraciones    =  PreProcesarOraciones(corpus)\n",
        "CorpusConClave  = CrearDiccionario(corpus,docID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DAyV3uSzSRv"
      },
      "source": [
        "Entrenamos el modelo en base a las oraciones generadas previamente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uIh2CDtDzY_o"
      },
      "outputs": [],
      "source": [
        "EntrenarModelo(oraciones,'mi_word2vec')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovHOhFoJzbEQ"
      },
      "source": [
        "Luego, cargamos el modelo entrenado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NWBtEpdgzlYA"
      },
      "outputs": [],
      "source": [
        "modelo, vocabulario = CargarModelo('mi_word2vec')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3OglWudfpPk"
      },
      "source": [
        "Podemos, obtener el embedding de alguna palabra:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-fqrq_W0sFw"
      },
      "source": [
        "Una vez que tenemos nuestro modelo cargado, podemos realizar diferentes tareas sobre los vectores de palabras u oraciones.\n",
        "\n",
        "Por ejemplo, podemos determinar la *cercanía* entre dos documentos del corpus. Para ello:\n",
        "\n",
        "1.   Tomamos el texto de cada documento.\n",
        "2.   Obtenemos sus respectivos vectores (embeddings).\n",
        "3.   Calculamos la distancia **coseno**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HwRvrMqgG-d"
      },
      "source": [
        "Note que la calidad del resultado depende de la cercanía real de los documentos!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Solicitamos al Usuario las palabras para comparar y mostramos las primeras 30 similitudes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documento: 149691.txt, Similitud: 0.6409827470779419\n",
            "Documento: 149471.txt, Similitud: 0.625017523765564\n",
            "Documento: 150206.txt, Similitud: 0.6183123588562012\n",
            "Documento: 97464.txt, Similitud: 0.6145167946815491\n",
            "Documento: 152647.txt, Similitud: 0.6103097200393677\n",
            "Documento: 135384.txt, Similitud: 0.6055464744567871\n",
            "Documento: 84418.txt, Similitud: 0.6054936051368713\n",
            "Documento: 151662.txt, Similitud: 0.6044436693191528\n",
            "Documento: 82199.txt, Similitud: 0.6034389138221741\n",
            "Documento: 85692.txt, Similitud: 0.6033058762550354\n",
            "Documento: 103681.txt, Similitud: 0.6029363870620728\n",
            "Documento: 82440.txt, Similitud: 0.6006869673728943\n",
            "Documento: 103651.txt, Similitud: 0.6001328825950623\n",
            "Documento: 74127.txt, Similitud: 0.5995113253593445\n",
            "Documento: 134150.txt, Similitud: 0.5983396172523499\n",
            "Documento: 152727.txt, Similitud: 0.5983177423477173\n",
            "Documento: 151583.txt, Similitud: 0.5976815819740295\n",
            "Documento: 150645.txt, Similitud: 0.5976508855819702\n",
            "Documento: 151937.txt, Similitud: 0.597638726234436\n",
            "Documento: 72656.txt, Similitud: 0.5969370603561401\n",
            "Documento: 149210.txt, Similitud: 0.5945630669593811\n",
            "Documento: 152444.txt, Similitud: 0.5944116711616516\n",
            "Documento: 81050.txt, Similitud: 0.5930393934249878\n",
            "Documento: 164282.txt, Similitud: 0.5930287837982178\n",
            "Documento: 99120.txt, Similitud: 0.5917245149612427\n",
            "Documento: 152967.txt, Similitud: 0.590538740158081\n",
            "Documento: 137858.txt, Similitud: 0.5903235673904419\n",
            "Documento: 150322.txt, Similitud: 0.5895110368728638\n",
            "Documento: 81517.txt, Similitud: 0.5888689160346985\n",
            "Documento: 150341.txt, Similitud: 0.5883181691169739\n"
          ]
        }
      ],
      "source": [
        "\n",
        "query = input(\"Ingresa tus palabras para comparar: \")\n",
        "\n",
        "\n",
        "query_embedding = ObtenerEmbeddingOracion(modelo, query)\n",
        "\n",
        "\n",
        "similitud_puntajes = []\n",
        "for documentos in corpus:\n",
        "    documentos_embedding = ObtenerEmbeddingOracion(modelo, documentos)\n",
        "    similitud = 1 - cosine(query_embedding, documentos_embedding)\n",
        "    similitud_puntajes.append(similitud)\n",
        "\n",
        "\n",
        "top_indices = np.argsort(similitud_puntajes)[::-1][:30]\n",
        "\n",
        "\n",
        "for index in top_indices:\n",
        "    documentos_name = docID[index]\n",
        "    similitud = similitud_puntajes[index]\n",
        "    print(f\"Documento: {documentos_name}, Similitud: {similitud}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
